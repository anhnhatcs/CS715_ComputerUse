#!/usr/bin/env python3
"""
Log analyzer for Claude Computer Use Demo
This script analyzes log files generated by the Streamlit application and creates summaries for evaluation.
"""

import os
import re
import json
import argparse
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime
from collections import Counter

def parse_log_file(log_path):
    """Parse a log file and extract structured data."""
    data = []
    
    with open(log_path, 'r') as f:
        log_content = f.read()
    
    # Extract all log entries
    log_entries = re.findall(r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2},\d{3}) \[(\w+)\] (.*)', log_content)
    
    for timestamp, level, message in log_entries:
        entry = {
            'timestamp': datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S,%f'),
            'level': level,
            'message': message
        }
        
        # Extract specific data based on message patterns
        if 'User entered instruction:' in message:
            entry['type'] = 'user_instruction'
            entry['instruction'] = message.replace('User entered instruction: ', '')
        
        elif 'Run button clicked with instruction:' in message:
            entry['type'] = 'run_button'
            entry['instruction'] = re.search(r"Run button clicked with instruction: '(.*)'", message).group(1)
        
        elif 'Assistant output:' in message:
            entry['type'] = 'assistant_output'
            entry['output'] = message.replace('Assistant output: ', '')
        
        elif 'Tool Output' in message:
            entry['type'] = 'tool_output'
            match = re.search(r"> Tool Output \[(.*?)\]: (.*)", message)
            if match:
                entry['tool_id'] = match.group(1)
                entry['output'] = match.group(2)
        
        elif 'Tool Error' in message:
            entry['type'] = 'tool_error'
            match = re.search(r"!!! Tool Error \[(.*?)\]: (.*)", message)
            if match:
                entry['tool_id'] = match.group(1)
                entry['error'] = match.group(2)
        
        elif 'Screenshot saved:' in message:
            entry['type'] = 'screenshot'
            entry['path'] = message.replace('Screenshot saved: ', '')
        
        elif 'API Response:' in message:
            entry['type'] = 'api_response'
            try:
                # Extract the JSON part of the message
                json_str = message.replace('API Response: ', '')
                entry['response'] = json.loads(json_str)
            except json.JSONDecodeError:
                entry['response'] = message.replace('API Response: ', '')
        
        elif 'Error during sampling loop:' in message:
            entry['type'] = 'error'
            entry['error'] = message.replace('Error during sampling loop: ', '')
        
        else:
            entry['type'] = 'other'
        
        data.append(entry)
    
    return data

def generate_summary(data):
    """Generate summary statistics from the parsed log data."""
    summary = {}
    
    # Basic counts
    summary['total_entries'] = len(data)
    summary['entry_types'] = Counter([entry['type'] for entry in data])
    summary['error_count'] = summary['entry_types'].get('error', 0) + summary['entry_types'].get('tool_error', 0)
    
    # User instructions
    user_instructions = [entry for entry in data if entry['type'] == 'user_instruction']
    summary['user_instruction_count'] = len(user_instructions)
    
    # Assistant outputs
    assistant_outputs = [entry for entry in data if entry['type'] == 'assistant_output']
    summary['assistant_output_count'] = len(assistant_outputs)
    
    # Tool usages
    tool_outputs = [entry for entry in data if entry['type'] == 'tool_output']
    summary['tool_output_count'] = len(tool_outputs)
    
    # Screenshots
    screenshots = [entry for entry in data if entry['type'] == 'screenshot']
    summary['screenshot_count'] = len(screenshots)
    
    # Session duration if possible
    if data:
        start_time = min(entry['timestamp'] for entry in data)
        end_time = max(entry['timestamp'] for entry in data)
        duration = (end_time - start_time).total_seconds()
        summary['session_duration_seconds'] = duration
        summary['session_start'] = start_time.isoformat()
        summary['session_end'] = end_time.isoformat()
    
    return summary

def create_visualizations(data, output_dir):
    """Create visualizations from the parsed log data."""
    os.makedirs(output_dir, exist_ok=True)
    
    # Convert to DataFrame for easier manipulation
    df = pd.DataFrame(data)
    
    # 1. Message types distribution
    if 'type' in df.columns:
        type_counts = df['type'].value_counts()
        plt.figure(figsize=(10, 6))
        type_counts.plot(kind='bar')
        plt.title('Distribution of Message Types')
        plt.xlabel('Message Type')
        plt.ylabel('Count')
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'message_types.png'))
        plt.close()
    
    # 2. Log levels distribution
    if 'level' in df.columns:
        level_counts = df['level'].value_counts()
        plt.figure(figsize=(8, 6))
        level_counts.plot(kind='pie', autopct='%1.1f%%')
        plt.title('Distribution of Log Levels')
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'log_levels.png'))
        plt.close()
    
    # 3. Timeline of events
    if 'timestamp' in df.columns and 'type' in df.columns:
        plt.figure(figsize=(12, 6))
        for event_type in df['type'].unique():
            type_df = df[df['type'] == event_type]
            plt.scatter(type_df['timestamp'], [event_type] * len(type_df), label=event_type, alpha=0.7)
        
        plt.yticks(df['type'].unique())
        plt.title('Timeline of Events')
        plt.xlabel('Time')
        plt.ylabel('Event Type')
        plt.legend()
        plt.grid(axis='y')
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'event_timeline.png'))
        plt.close()

def main():
    parser = argparse.ArgumentParser(description='Analyze Claude Computer Use logs')
    parser.add_argument('log_file', help='Path to the log file to analyze')
    parser.add_argument('--output-dir', '-o', default='log_analysis', help='Directory to save analysis results')
    
    args = parser.parse_args()
    
    if not os.path.isfile(args.log_file):
        print(f"Error: Log file {args.log_file} not found.")
        return
    
    # Create output directory
    os.makedirs(args.output_dir, exist_ok=True)
    
    # Parse the log file
    print(f"Parsing log file: {args.log_file}")
    log_data = parse_log_file(args.log_file)
    
    # Generate summary
    print("Generating summary...")
    summary = generate_summary(log_data)
    
    # Save summary to a JSON file
    summary_path = os.path.join(args.output_dir, 'summary.json')
    with open(summary_path, 'w') as f:
        json.dump(summary, f, indent=4)
    
    print(f"Summary saved to: {summary_path}")
    
    # Create visualizations
    print("Creating visualizations...")
    create_visualizations(log_data, args.output_dir)
    
    print(f"Analysis complete. Results saved to: {args.output_dir}")
    
    # Print key statistics
    print("\nKey Statistics:")
    print(f"Total log entries: {summary['total_entries']}")
    print(f"Session duration: {summary.get('session_duration_seconds', 'N/A')} seconds")
    print(f"User instructions: {summary.get('user_instruction_count', 0)}")
    print(f"Assistant outputs: {summary.get('assistant_output_count', 0)}")
    print(f"Tool uses: {summary.get('tool_output_count', 0)}")
    print(f"Errors: {summary.get('error_count', 0)}")
    print(f"Screenshots captured: {summary.get('screenshot_count', 0)}")

if __name__ == "__main__":
    main()
