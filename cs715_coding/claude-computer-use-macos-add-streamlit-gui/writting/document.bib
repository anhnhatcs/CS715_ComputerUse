
@article{wang_survey_2024,
	title = {A {Survey} on {Large} {Language} {Model} based {Autonomous} {Agents}},
	volume = {18},
	issn = {2095-2228, 2095-2236},
	url = {http://arxiv.org/abs/2308.11432},
	doi = {10.1007/s11704-024-40231-1},
	abstract = {Autonomous agents have long been a prominent research focus in both academic and industry communities. Previous research in this field often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from human learning processes, and thus makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have demonstrated remarkable potential in achieving human-level intelligence. This has sparked an upsurge in studies investigating LLM-based autonomous agents. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of the field of LLM-based autonomous agents from a holistic perspective. More specifically, we first discuss the construction of LLM-based autonomous agents, for which we propose a unified framework that encompasses a majority of the previous work. Then, we present a comprehensive overview of the diverse applications of LLM-based autonomous agents in the fields of social science, natural science, and engineering. Finally, we delve into the evaluation strategies commonly used for LLM-based autonomous agents. Based on the previous studies, we also present several challenges and future directions in this field. To keep track of this field and continuously update our survey, we maintain a repository of relevant references at https://github.com/Paitesanshi/LLM-Agent-Survey.},
	language = {en},
	number = {6},
	urldate = {2025-03-12},
	journal = {Frontiers of Computer Science},
	author = {Wang, Lei and Ma, Chen and Feng, Xueyang and Zhang, Zeyu and Yang, Hao and Zhang, Jingsen and Chen, Zhiyuan and Tang, Jiakai and Chen, Xu and Lin, Yankai and Zhao, Wayne Xin and Wei, Zhewei and Wen, Ji-Rong},
	month = dec,
	year = {2024},
	note = {arXiv:2308.11432 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	pages = {186345},
	annote = {Comment: Correcting several typos, 35 pages, 5 figures, 3 tables},
	file = {Wang et al. - 2024 - A Survey on Large Language Model based Autonomous .pdf:/Users/anhnhat/Zotero/storage/2HTY8ZR3/Wang et al. - 2024 - A Survey on Large Language Model based Autonomous .pdf:application/pdf},
}

@misc{sager_ai_2025,
	title = {{AI} {Agents} for {Computer} {Use}: {A} {Review} of {Instruction}-based {Computer} {Control}, {GUI} {Automation}, and {Operator} {Assistants}},
	shorttitle = {{AI} {Agents} for {Computer} {Use}},
	url = {http://arxiv.org/abs/2501.16150},
	doi = {10.48550/arXiv.2501.16150},
	abstract = {Instruction-based computer control agents (CCAs) execute complex action sequences on personal computers or mobile devices to fulfill tasks using the same graphical user interfaces as a human user would, provided instructions in natural language. This review offers a comprehensive overview of the emerging field of instruction-based computer control, examining available agents -- their taxonomy, development, and respective resources -- and emphasizing the shift from manually designed, specialized agents to leveraging foundation models such as large language models (LLMs) and vision-language models (VLMs). We formalize the problem and establish a taxonomy of the field to analyze agents from three perspectives: (a) the environment perspective, analyzing computer environments; (b) the interaction perspective, describing observations spaces (e.g., screenshots, HTML) and action spaces (e.g., mouse and keyboard actions, executable code); and (c) the agent perspective, focusing on the core principle of how an agent acts and learns to act. Our framework encompasses both specialized and foundation agents, facilitating their comparative analysis and revealing how prior solutions in specialized agents, such as an environment learning step, can guide the development of more capable foundation agents. Additionally, we review current CCA datasets and CCA evaluation methods and outline the challenges to deploying such agents in a productive setting. In total, we review and classify 86 CCAs and 33 related datasets. By highlighting trends, limitations, and future research directions, this work presents a comprehensive foundation to obtain a broad understanding of the field and push its future development.},
	language = {en},
	urldate = {2025-03-12},
	publisher = {arXiv},
	author = {Sager, Pascal J. and Meyer, Benjamin and Yan, Peng and Wartburg-Kottler, Rebekka von and Etaiwi, Layan and Enayati, Aref and Nobel, Gabriel and Abdulkadir, Ahmed and Grewe, Benjamin F. and Stadelmann, Thilo},
	month = jan,
	year = {2025},
	note = {arXiv:2501.16150 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Systems and Control, Electrical Engineering and Systems Science - Systems and Control},
	file = {Sager et al. - 2025 - AI Agents for Computer Use A Review of Instructio.pdf:/Users/anhnhat/Zotero/storage/FLKZXBGR/Sager et al. - 2025 - AI Agents for Computer Use A Review of Instructio.pdf:application/pdf},
}

@misc{luo_autellix_2025,
	title = {Autellix: {An} {Efficient} {Serving} {Engine} for {LLM} {Agents} as {General} {Programs}},
	shorttitle = {Autellix},
	url = {http://arxiv.org/abs/2502.13965},
	doi = {10.48550/arXiv.2502.13965},
	abstract = {Large language model (LLM) applications are evolving beyond simple chatbots into dynamic, general-purpose agentic programs, which scale LLM calls and output tokens to help AI agents reason, explore, and solve complex tasks. However, existing LLM serving systems ignore dependencies between programs and calls, missing significant opportunities for optimization. Our analysis reveals that programs submitted to LLM serving engines experience long cumulative wait times, primarily due to head-of-line blocking at both the individual LLM request and the program. To address this, we introduce Autellix, an LLM serving system that treats programs as first-class citizens to minimize their end-to-end latencies. Autellix intercepts LLM calls submitted by programs, enriching schedulers with programlevel context. We propose two scheduling algorithms—for single-threaded and distributed programs—that preempt and prioritize LLM calls based on their programs’ previously completed calls. Our evaluation demonstrates that across diverse LLMs and agentic workloads, Autellix improves throughput of programs by 4-15× at the same latency compared to state-of-the-art systems, such as vLLM.},
	language = {en},
	urldate = {2025-03-12},
	publisher = {arXiv},
	author = {Luo, Michael and Shi, Xiaoxiang and Cai, Colin and Zhang, Tianjun and Wong, Justin and Wang, Yichuan and Wang, Chi and Huang, Yanping and Chen, Zhifeng and Gonzalez, Joseph E. and Stoica, Ion},
	month = feb,
	year = {2025},
	note = {arXiv:2502.13965 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning},
	file = {Luo et al. - 2025 - Autellix An Efficient Serving Engine for LLM Agen.pdf:/Users/anhnhat/Zotero/storage/475KH6F7/Luo et al. - 2025 - Autellix An Efficient Serving Engine for LLM Agen.pdf:application/pdf},
}

@misc{huq_cowpilot_2025,
	title = {{CowPilot}: {A} {Framework} for {Autonomous} and {Human}-{Agent} {Collaborative} {Web} {Navigation}},
	shorttitle = {{CowPilot}},
	url = {http://arxiv.org/abs/2501.16609},
	doi = {10.48550/arXiv.2501.16609},
	abstract = {While much work on web agents emphasizes the promise of autonomously performing tasks on behalf of users, in reality, agents often fall short on complex tasks in real-world contexts and modeling user preference. This presents an opportunity for humans to collaborate with the agent and leverage the agent's capabilities effectively. We propose CowPilot, a framework supporting autonomous as well as human-agent collaborative web navigation, and evaluation across task success and task efficiency. CowPilot reduces the number of steps humans need to perform by allowing agents to propose next steps, while users are able to pause, reject, or take alternative actions. During execution, users can interleave their actions with the agent by overriding suggestions or resuming agent control when needed. We conducted case studies on five common websites and found that the human-agent collaborative mode achieves the highest success rate of 95\% while requiring humans to perform only 15.2\% of the total steps. Even with human interventions during task execution, the agent successfully drives up to half of task success on its own. CowPilot can serve as a useful tool for data collection and agent evaluation across websites, which we believe will enable research in how users and agents can work together. Video demonstrations are available at https://oaishi.github.io/cowpilot.html},
	language = {en},
	urldate = {2025-03-12},
	publisher = {arXiv},
	author = {Huq, Faria and Wang, Zora Zhiruo and Xu, Frank F. and Ou, Tianyue and Zhou, Shuyan and Bigham, Jeffrey P. and Neubig, Graham},
	month = feb,
	year = {2025},
	note = {arXiv:2501.16609 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},
	annote = {Comment: Preprint},
	file = {Huq et al. - 2025 - CowPilot A Framework for Autonomous and Human-Age.pdf:/Users/anhnhat/Zotero/storage/2HPS76D4/Huq et al. - 2025 - CowPilot A Framework for Autonomous and Human-Age.pdf:application/pdf},
}

@misc{steiner_fine-tuning_2024,
	title = {Fine-tuning {Large} {Language} {Models} for {Entity} {Matching}},
	url = {http://arxiv.org/abs/2409.08185},
	doi = {10.48550/arXiv.2409.08185},
	abstract = {Generative large language models (LLMs) are a promising alternative to pre-trained language models for entity matching due to their high zero-shot performance and their ability to generalize to unseen entities. Existing research on using LLMs for entity matching has focused on prompt engineering and in-context learning. This paper explores the potential of fine-tuning LLMs for entity matching. We analyze fine-tuning along two dimensions: 1) The representation of training examples, where we experiment with adding different types of LLM-generated explanations to the training set, and 2) the selection and generation of training examples using LLMs. In addition to the matching performance on the source dataset, we investigate how fine-tuning affects the model’s ability to generalize to other in-domain datasets as well as across topical domains. Our experiments show that fine-tuning significantly improves the performance of the smaller models while the results for the larger models are mixed. Fine-tuning also improves the generalization to in-domain datasets while hurting cross-domain transfer. We show that adding structured explanations to the training set has a positive impact on the performance of three out of four LLMs, while the proposed example selection and generation methods only improve the performance of Llama 3.1 8B while decreasing the performance of GPT-4o Mini.},
	language = {en},
	urldate = {2025-03-12},
	publisher = {arXiv},
	author = {Steiner, Aaron and Peeters, Ralph and Bizer, Christian},
	month = sep,
	year = {2024},
	note = {arXiv:2409.08185 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 8 pages, 4 figures. For related code and data, see this https://github.com/wbsg-uni-mannheim/TailorMatch},
	file = {Steiner et al. - 2024 - Fine-tuning Large Language Models for Entity Match.pdf:/Users/anhnhat/Zotero/storage/GK66L2D9/Steiner et al. - 2024 - Fine-tuning Large Language Models for Entity Match.pdf:application/pdf},
}

@misc{nguyen_gui_2024,
	title = {{GUI} {Agents}: {A} {Survey}},
	shorttitle = {{GUI} {Agents}},
	url = {http://arxiv.org/abs/2412.13501},
	doi = {10.48550/arXiv.2412.13501},
	abstract = {Graphical User Interface (GUI) agents, powered by Large Foundation Models, have emerged as a transformative approach to automating human-computer interaction. These agents autonomously interact with digital systems or software applications via GUIs, emulating human actions such as clicking, typing, and navigating visual elements across diverse platforms. Motivated by the growing interest and fundamental importance of GUI agents, we provide a comprehensive survey that categorizes their benchmarks, evaluation metrics, architectures, and training methods. We propose a unified framework that delineates their perception, reasoning, planning, and acting capabilities. Furthermore, we identify important open challenges and discuss key future directions. Finally, this work serves as a basis for practitioners and researchers to gain an intuitive understanding of current progress, techniques, benchmarks, and critical open problems that remain to be addressed.},
	language = {en},
	urldate = {2025-03-12},
	publisher = {arXiv},
	author = {Nguyen, Dang and Chen, Jian and Wang, Yu and Wu, Gang and Park, Namyong and Hu, Zhengmian and Lyu, Hanjia and Wu, Junda and Aponte, Ryan and Xia, Yu and Li, Xintong and Shi, Jing and Chen, Hongjie and Lai, Viet Dac and Xie, Zhouhang and Kim, Sungchul and Zhang, Ruiyi and Yu, Tong and Tanjim, Mehrab and Ahmed, Nesreen K. and Mathur, Puneet and Yoon, Seunghyun and Yao, Lina and Kveton, Branislav and Nguyen, Thien Huu and Bui, Trung and Zhou, Tianyi and Rossi, Ryan A. and Dernoncourt, Franck},
	month = dec,
	year = {2024},
	note = {arXiv:2412.13501 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
	file = {Nguyen et al. - 2024 - GUI Agents A Survey.pdf:/Users/anhnhat/Zotero/storage/JPDLEGD6/Nguyen et al. - 2024 - GUI Agents A Survey.pdf:application/pdf},
}

@misc{nathani_mlgym_2025,
	title = {{MLGym}: {A} {New} {Framework} and {Benchmark} for {Advancing} {AI} {Research} {Agents}},
	shorttitle = {{MLGym}},
	url = {http://arxiv.org/abs/2502.14499},
	doi = {10.48550/arXiv.2502.14499},
	abstract = {We introduce Meta MLGym and MLGym-Bench, a new framework and benchmark for evaluating and developing LLM agents on AI research tasks. This is the first Gym environment for machine learning (ML) tasks, enabling research on reinforcement learning (RL) algorithms for training such agents. MLGym-bench consists of 13 diverse and open-ended AI research tasks from diverse domains such as computer vision, natural language processing, reinforcement learning, and game theory. Solving these tasks requires real-world AI research skills such as generating new ideas and hypotheses, creating and processing data, implementing ML methods, training models, running experiments, analyzing the results, and iterating through this process to improve on a given task. We evaluate a number of frontier large language models (LLMs) on our benchmarks such as Claude-3.5-Sonnet, Llama-3.1 405B, GPT-4o, o1-preview, and Gemini-1.5 Pro. Our MLGym framework makes it easy to add new tasks, integrate and evaluate models or agents, generate synthetic data at scale, as well as develop new learning algorithms for training agents on AI research tasks. We find that current frontier models can improve on the given baselines, usually by finding better hyperparameters, but do not generate novel hypotheses, algorithms, architectures, or substantial improvements. We open-source our framework and benchmark to facilitate future research in advancing the AI research capabilities of LLM agents.},
	language = {en},
	urldate = {2025-03-12},
	publisher = {arXiv},
	author = {Nathani, Deepak and Madaan, Lovish and Roberts, Nicholas and Bashlykov, Nikolay and Menon, Ajay and Moens, Vincent and Budhiraja, Amar and Magka, Despoina and Vorotilov, Vladislav and Chaurasia, Gaurav and Hupkes, Dieuwke and Cabral, Ricardo Silveira and Shavrina, Tatiana and Foerster, Jakob and Bachrach, Yoram and Wang, William Yang and Raileanu, Roberta},
	month = feb,
	year = {2025},
	note = {arXiv:2502.14499 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 35 pages, 12 figures, 10 tables},
	file = {Nathani et al. - 2025 - MLGym A New Framework and Benchmark for Advancing.pdf:/Users/anhnhat/Zotero/storage/6X6RBAZ2/Nathani et al. - 2025 - MLGym A New Framework and Benchmark for Advancing.pdf:application/pdf},
}

@misc{lu_omniparser_2024,
	title = {{OmniParser} for {Pure} {Vision} {Based} {GUI} {Agent}},
	url = {http://arxiv.org/abs/2408.00203},
	doi = {10.48550/arXiv.2408.00203},
	abstract = {The recent success of large vision language models shows great potential in driving the agent system operating on user interfaces. However, we argue that the power multimodal models like GPT-4V as a general agent on multiple operating systems across different applications is largely underestimated due to the lack of a robust screen parsing technique capable of: 1) reliably identifying interactable icons within the user interface, and 2) understanding the semantics of various elements in a screenshot and accurately associate the intended action with the corresponding region on the screen. To fill these gaps, we introduce OMNIPARSER, a comprehensive method for parsing user interface screenshots into structured elements, which significantly enhances the ability of GPT-4V to generate actions that can be accurately grounded in the corresponding regions of the interface. We first curated an interactable icon detection dataset using popular webpages and an icon description dataset. These datasets were utilized to fine-tune specialized models: a detection model to parse interactable regions on the screen and a caption model to extract the functional semantics of the detected elements. OMNIPARSER significantly improves GPT-4V’s performance on ScreenSpot benchmark. And on Mind2Web and AITW benchmark, OMNIPARSER with screenshot only input outperforms the GPT-4V baselines requiring additional information outside of screenshot.},
	language = {en},
	urldate = {2025-03-12},
	publisher = {arXiv},
	author = {Lu, Yadong and Yang, Jianwei and Shen, Yelong and Awadallah, Ahmed},
	month = aug,
	year = {2024},
	note = {arXiv:2408.00203 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Lu et al. - 2024 - OmniParser for Pure Vision Based GUI Agent.pdf:/Users/anhnhat/Zotero/storage/6GU94ICV/Lu et al. - 2024 - OmniParser for Pure Vision Based GUI Agent.pdf:application/pdf},
}

@misc{yu_omniparser_2025,
	title = {{OmniParser} {V2}: {Structured}-{Points}-of-{Thought} for {Unified} {Visual} {Text} {Parsing} and {Its} {Generality} to {Multimodal} {Large} {Language} {Models}},
	shorttitle = {{OmniParser} {V2}},
	url = {http://arxiv.org/abs/2502.16161},
	doi = {10.48550/arXiv.2502.16161},
	abstract = {Visually-situated text parsing (VsTP) has recently seen notable advancements, driven by the growing demand for automated document understanding and the emergence of large language models capable of processing document-based questions. While various methods have been proposed to tackle the complexities of VsTP, existing solutions often rely on task-specific architectures and objectives for individual tasks. This leads to modal isolation and complex workflows due to the diversified targets and heterogeneous schemas. In this paper, we introduce OmniParser V2, a universal model that unifies VsTP typical tasks, including text spotting, key information extraction, table recognition, and layout analysis, into a unified framework. Central to our approach is the proposed Structured-Points-of-Thought (SPOT) prompting schemas, which improves model performance across diverse scenarios by leveraging a unified encoder-decoder architecture, objective, and input\&output representation. SPOT eliminates the need for task-specific architectures and loss functions, significantly simplifying the processing pipeline. Our extensive evaluations across four tasks on eight different datasets show that OmniParser V2 achieves state-of-the-art or competitive results in VsTP. Additionally, we explore the integration of SPOT within a multimodal large language model structure, further enhancing text localization and recognition capabilities, thereby confirming the generality of SPOT prompting technique. The code is available at AdvancedLiterateMachinery.},
	language = {en},
	urldate = {2025-03-12},
	publisher = {arXiv},
	author = {Yu, Wenwen and Yang, Zhibo and Wan, Jianqiang and Song, Sibo and Tang, Jun and Cheng, Wenqing and Liu, Yuliang and Bai, Xiang},
	month = feb,
	year = {2025},
	note = {arXiv:2502.16161 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {Yu et al. - 2025 - OmniParser V2 Structured-Points-of-Thought for Un.pdf:/Users/anhnhat/Zotero/storage/8X7YA759/Yu et al. - 2025 - OmniParser V2 Structured-Points-of-Thought for Un.pdf:application/pdf},
}

@misc{liu_pc-agent_2025,
	title = {{PC}-{Agent}: {A} {Hierarchical} {Multi}-{Agent} {Collaboration} {Framework} for {Complex} {Task} {Automation} on {PC}},
	shorttitle = {{PC}-{Agent}},
	url = {http://arxiv.org/abs/2502.14282},
	doi = {10.48550/arXiv.2502.14282},
	abstract = {In the field of MLLM-based GUI agents, compared to smartphones, the PC scenario not only features a more complex interactive environment, but also involves more intricate intraand inter-app workflows. To address these issues, we propose a hierarchical agent framework named PC-Agent. Specifically, from the perception perspective, we devise an Active Perception Module (APM) to overcome the inadequate abilities of current MLLMs in perceiving screenshot content. From the decision-making perspective, to handle complex user instructions and interdependent subtasks more effectively, we propose a hierarchical multi-agent collaboration architecture that decomposes decision-making processes into Instruction-Subtask-Action levels. Within this architecture, three agents (i.e., Manager, Progress and Decision) are set up for instruction decomposition, progress tracking and stepby-step decision-making respectively. Additionally, a Reflection agent is adopted to enable timely bottom-up error feedback and adjustment. We also introduce a new benchmark PC-Eval with 25 real-world complex instructions. Empirical results on PC-Eval show that our PC-Agent achieves a 32\% absolute improvement of task success rate over previous state-of-the-art methods. The code is available at https://github.com/X-PLUG/ MobileAgent/tree/main/PC-Agent.},
	language = {en},
	urldate = {2025-03-12},
	publisher = {arXiv},
	author = {Liu, Haowei and Zhang, Xi and Xu, Haiyang and Wanyan, Yuyang and Wang, Junyang and Yan, Ming and Zhang, Ji and Yuan, Chunfeng and Xu, Changsheng and Hu, Weiming and Huang, Fei},
	month = feb,
	year = {2025},
	note = {arXiv:2502.14282 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 14 pages, 7 figures},
	file = {Liu et al. - 2025 - PC-Agent A Hierarchical Multi-Agent Collaboration.pdf:/Users/anhnhat/Zotero/storage/8KNY524L/Liu et al. - 2025 - PC-Agent A Hierarchical Multi-Agent Collaboration.pdf:application/pdf},
}

@article{chang_survey_2024,
	title = {A {Survey} on {Evaluation} of {Large} {Language} {Models}},
	volume = {15},
	issn = {2157-6904},
	url = {https://dl.acm.org/doi/10.1145/3641289},
	doi = {10.1145/3641289},
	abstract = {Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, education, natural and social sciences, agent applications, and other areas. Secondly, we answer the ‘where’ and ‘how’ questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing the performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at:},
	number = {3},
	urldate = {2025-03-12},
	journal = {ACM Trans. Intell. Syst. Technol.},
	author = {Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and Ye, Wei and Zhang, Yue and Chang, Yi and Yu, Philip S. and Yang, Qiang and Xie, Xing},
	month = mar,
	year = {2024},
	pages = {39:1--39:45},
	file = {Full Text PDF:/Users/anhnhat/Zotero/storage/SM3QSVU2/Chang et al. - 2024 - A Survey on Evaluation of Large Language Models.pdf:application/pdf},
}

@misc{zhao_survey_2025,
	title = {A {Survey} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2303.18223},
	doi = {10.48550/arXiv.2303.18223},
	abstract = {Ever since the Turing Test was proposed in the 1950s, humans have explored the mastering of language intelligence by machine. Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable artificial intelligence (AI) algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pretraining Transformer models over large-scale corpora, showing strong capabilities in solving various natural language processing (NLP) tasks. Since the researchers have found that model scaling can lead to an improved model capacity, they further investigate the scaling effect by increasing the parameter scale to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement, but also exhibit some special abilities (e.g., incontext learning) that are not present in small-scale language models (e.g., BERT). To discriminate the language models in different parameter scales, the research community has coined the term large language models (LLM) for the PLMs of significant size (e.g., containing tens or hundreds of billions of parameters). Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT (a powerful AI chatbot developed based on LLMs), which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. Considering this rapid technical progress, in this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Furthermore, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions. This survey provides an up-to-date review of the literature on LLMs, which can be a useful resource for both researchers and engineers.},
	language = {en},
	urldate = {2025-03-12},
	publisher = {arXiv},
	author = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and Du, Yifan and Yang, Chen and Chen, Yushuo and Chen, Zhipeng and Jiang, Jinhao and Ren, Ruiyang and Li, Yifan and Tang, Xinyu and Liu, Zikang and Liu, Peiyu and Nie, Jian-Yun and Wen, Ji-Rong},
	month = mar,
	year = {2025},
	note = {arXiv:2303.18223 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: ongoing work; 144 pages, 1081 citations},
	file = {Zhao et al. - 2025 - A Survey of Large Language Models.pdf:/Users/anhnhat/Zotero/storage/HRY8KT7P/Zhao et al. - 2025 - A Survey of Large Language Models.pdf:application/pdf},
}

@inproceedings{yao_-bench_2024,
	title = {\$τ\$-bench: {A} {Benchmark} for {Tool}-{Agent}-{User} {Interaction} in {Real}-{World} {Domains}},
	shorttitle = {\{\${\textbackslash}tau\$\}-bench},
	url = {https://openreview.net/forum?id=roNSXZpUDN},
	abstract = {Existing benchmarks for language agents do not set them up to interact with human users or follow domain-specific rules, both of which are vital to safe and realistic deployment. We propose \${\textbackslash}tau\$-bench, a benchmark with two domains (retail and airline) emulating dynamic conversations between a user (simulated by language models) and a customer service agent provided with domain-specific API tools and policy guidelines. We employ a efficient and faithful evaluation process that compares the database state at the end of a conversation with the annotated goal state, and propose a new metric (pass{\textasciicircum}k) to evaluate the reliability of agent behavior over multiple trials. Our experiments show that even state-of-the-art function calling agents (gpt-4o) succeed on \${\textless}50{\textbackslash}\%\$ of the tasks, and are terribly inconsistent (pass{\textasciicircum}8 {\textless} 25{\textbackslash}\% in retail). Our findings point to the need for methods that can improve the ability of agents to act consistently and reliably follow rules.},
	language = {en},
	urldate = {2025-03-12},
	author = {Yao, Shunyu and Shinn, Noah and Razavi, Pedram and Narasimhan, Karthik R.},
	month = oct,
	year = {2024},
	file = {Full Text PDF:/Users/anhnhat/Zotero/storage/EY98AD7L/Yao et al. - 2024 - \$tau\$ -bench A Benchmark for underline T ool-.pdf:application/pdf},
}

@inproceedings{wu_os-atlas_2024,
	title = {{OS}-{ATLAS}: {Foundation} {Action} {Model} for {Generalist} {GUI} {Agents}},
	shorttitle = {{OS}-{ATLAS}},
	url = {https://openreview.net/forum?id=n9PDaFNi8t},
	abstract = {Existing efforts in building GUI agents heavily rely on the availability of robust commercial Vision-Language Models (VLMs) such as GPT-4o and GeminiProVision. Practitioners are often reluctant to use open-source VLMs due to their significant performance lag compared to their closed-source counterparts, particularly in GUI grounding and Out-Of-Distribution (OOD) scenarios. To facilitate future research in this area, we developed OS-Atlas—a foundational GUI action model that excels at GUI grounding and OOD agentic tasks through innovations in both data and modeling. We have invested significant engineering effort in developing an open-source toolkit for synthesizing GUI grounding data across multiple platforms, including Windows, Linux, MacOS, Android, and the web. Leveraging this toolkit, we are releasing the largest open-source cross-platform GUI grounding corpus to date, which contains over 13 million GUI elements. This dataset, combined with innovations in model training, provides a solid foundation for OS-Atlas to understand GUI screenshots and generalize to unseen interfaces. Through extensive evaluation across six benchmarks spanning three different platforms (mobile, desktop, and web), OS-Atlas demonstrates significant performance improvements over previous state-of-the-art models. Our evaluation also uncovers valuable insights into continuously improving and scaling the agentic capabilities of open-source VLMs.},
	language = {en},
	urldate = {2025-03-12},
	author = {Wu, Zhiyong and Wu, Zhenyu and Xu, Fangzhi and Wang, Yian and Sun, Qiushi and Jia, Chengyou and Cheng, Kanzhi and Ding, Zichen and Chen, Liheng and Liang, Paul Pu and Qiao, Yu},
	month = oct,
	year = {2024},
	file = {Full Text PDF:/Users/anhnhat/Zotero/storage/F74Q87P8/Wu et al. - 2024 - OS-ATLAS Foundation Action Model for Generalist G.pdf:application/pdf},
}

@inproceedings{gou_navigating_2024,
	title = {Navigating the {Digital} {World} as {Humans} {Do}: {Universal} {Visual} {Grounding} for {GUI} {Agents}},
	shorttitle = {Navigating the {Digital} {World} as {Humans} {Do}},
	url = {https://openreview.net/forum?id=kxnoqaisCT},
	abstract = {Multimodal large language models (MLLMs) are transforming the capabilities of graphical user interface (GUI) agents, facilitating their transition from controlled simulations to complex, real-world applications across various platforms. However, the effectiveness of these agents hinges on the robustness of their grounding capability. Current GUI agents predominantly utilize text-based representations such as HTML or accessibility trees, which, despite their utility, often introduce noise, incompleteness, and increased computational overhead. In this paper, we advocate a human-like embodiment for GUI agents that perceive the environment entirely visually and directly perform pixel-level operations on the GUI. The key is visual grounding models that can accurately map diverse referring expressions of GUI elements to their coordinates on the GUI across different platforms. We show that a simple recipe, which includes web-based synthetic data and slight adaptation of the LLaVA architecture, is surprisingly effective for training such visual grounding models. We collect the largest dataset for GUI visual grounding so far, containing 10M GUI elements and their referring expressions over 1.3M screenshots, and use it to train UGround, a strong universal visual grounding model for GUI agents. Empirical results on six benchmarks spanning three categories (grounding, offline agent, and online agent) show that 1) UGround substantially outperforms existing visual grounding models for GUI agents, by up to 20{\textbackslash}\% absolute, and 2) agents with UGround outperform state-of-the-art agents, despite the fact that existing agents use additional text-based input while ours only uses visual perception. These results provide strong support for the feasibility and promises of GUI agents that navigate the digital world as humans do.},
	language = {en},
	urldate = {2025-03-12},
	author = {Gou, Boyu and Wang, Ruohan and Zheng, Boyuan and Xie, Yanan and Chang, Cheng and Shu, Yiheng and Sun, Huan and Su, Yu},
	month = oct,
	year = {2024},
	file = {Full Text PDF:/Users/anhnhat/Zotero/storage/XTHR79GC/Gou et al. - 2024 - Navigating the Digital World as Humans Do Univers.pdf:application/pdf},
}

@misc{hu_dawn_2024,
	title = {The {Dawn} of {GUI} {Agent}: {A} {Preliminary} {Case} {Study} with {Claude} 3.5 {Computer} {Use}},
	shorttitle = {The {Dawn} of {GUI} {Agent}},
	url = {http://arxiv.org/abs/2411.10323},
	doi = {10.48550/arXiv.2411.10323},
	abstract = {The recently released model, Claude 3.5 Computer Use, stands out as the first frontier AI model to offer computer use in public beta as a graphical user interface (GUI) agent. As an early beta, its capability in the real-world complex environment remains unknown. In this case study to explore Claude 3.5 Computer Use, we curate and organize a collection of carefully designed tasks spanning a variety of domains and software. Observations from these cases demonstrate Claude 3.5 Computer Use’s unprecedented ability in end-to-end language to desktop actions. Along with this study, we provide an out-of-the-box agent framework for deploying API-based GUI automation models with easy implementation. Our case studies aim to showcase a groundwork of capabilities and limitations of Claude 3.5 Computer Use with detailed analyses and bring to the fore questions about planning, action, and critic which must be considered for future improvement. We hope this preliminary exploration will inspire future research into the GUI agent community. All the test cases in the paper can be tried through the project: https://github.com/showlab/computer\_use\_ootb.},
	language = {en},
	urldate = {2025-03-12},
	publisher = {arXiv},
	author = {Hu, Siyuan and Ouyang, Mingyu and Gao, Difei and Shou, Mike Zheng},
	month = nov,
	year = {2024},
	note = {arXiv:2411.10323 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 40 pages, 21 figures, preprint},
	file = {Hu et al. - 2024 - The Dawn of GUI Agent A Preliminary Case Study wi.pdf:/Users/anhnhat/Zotero/storage/27LPHDYG/Hu et al. - 2024 - The Dawn of GUI Agent A Preliminary Case Study wi.pdf:application/pdf},
}

@misc{cheng_seeclick_2024,
	title = {{SeeClick}: {Harnessing} {GUI} {Grounding} for {Advanced} {Visual} {GUI} {Agents}},
	shorttitle = {{SeeClick}},
	url = {http://arxiv.org/abs/2401.10935},
	doi = {10.48550/arXiv.2401.10935},
	abstract = {Graphical User Interface (GUI) agents are designed to automate complex tasks on digital devices, such as smartphones and desktops. Most existing GUI agents interact with the environment through extracted structured data, which can be notably lengthy (e.g., HTML) and occasionally inaccessible (e.g., on desktops). To alleviate this issue, we propose a novel visual GUI agent -- SeeClick, which only relies on screenshots for task automation. In our preliminary study, we have discovered a key challenge in developing visual GUI agents: GUI grounding -- the capacity to accurately locate screen elements based on instructions. To tackle this challenge, we propose to enhance SeeClick with GUI grounding pre-training and devise a method to automate the curation of GUI grounding data. Along with the efforts above, we have also created ScreenSpot, the first realistic GUI grounding benchmark that encompasses mobile, desktop, and web environments. After pre-training, SeeClick demonstrates significant improvement in ScreenSpot over various baselines. Moreover, comprehensive evaluations on three widely used benchmarks consistently support our finding that advancements in GUI grounding directly correlate with enhanced performance in downstream GUI agent tasks. The model, data and code are available at https://github.com/njucckevin/SeeClick.},
	urldate = {2025-03-14},
	publisher = {arXiv},
	author = {Cheng, Kanzhi and Sun, Qiushi and Chu, Yougang and Xu, Fangzhi and Li, Yantao and Zhang, Jianbing and Wu, Zhiyong},
	month = feb,
	year = {2024},
	note = {arXiv:2401.10935 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
	file = {Full Text PDF:/Users/anhnhat/Zotero/storage/MGZD8Q2W/Cheng et al. - 2024 - SeeClick Harnessing GUI Grounding for Advanced Vi.pdf:application/pdf;Snapshot:/Users/anhnhat/Zotero/storage/JFZE3WUJ/2401.html:text/html},
}

@misc{deng_mind2web_2023,
	title = {{Mind2Web}: {Towards} a {Generalist} {Agent} for the {Web}},
	shorttitle = {{Mind2Web}},
	url = {http://arxiv.org/abs/2306.06070},
	doi = {10.48550/arXiv.2306.06070},
	abstract = {We introduce Mind2Web, the first dataset for developing and evaluating generalist agents for the web that can follow language instructions to complete complex tasks on any website. Existing datasets for web agents either use simulated websites or only cover a limited set of websites and tasks, thus not suitable for generalist web agents. With over 2,000 open-ended tasks collected from 137 websites spanning 31 domains and crowdsourced action sequences for the tasks, Mind2Web provides three necessary ingredients for building generalist web agents: 1) diverse domains, websites, and tasks, 2) use of real-world websites instead of simulated and simplified ones, and 3) a broad spectrum of user interaction patterns. Based on Mind2Web, we conduct an initial exploration of using large language models (LLMs) for building generalist web agents. While the raw HTML of real-world websites are often too large to be fed to LLMs, we show that first filtering it with a small LM significantly improves the effectiveness and efficiency of LLMs. Our solution demonstrates a decent level of performance, even on websites or entire domains the model has never seen before, but there is still a substantial room to improve towards truly generalizable agents. We open-source our dataset, model implementation, and trained models (https://osu-nlp-group.github.io/Mind2Web) to facilitate further research on building a generalist agent for the web.},
	urldate = {2025-03-14},
	publisher = {arXiv},
	author = {Deng, Xiang and Gu, Yu and Zheng, Boyuan and Chen, Shijie and Stevens, Samuel and Wang, Boshi and Sun, Huan and Su, Yu},
	month = dec,
	year = {2023},
	note = {arXiv:2306.06070 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Website: https://osu-nlp-group.github.io/Mind2Web. Updated with supplementary material. NeurIPS'23 Spotlight},
	file = {Preprint PDF:/Users/anhnhat/Zotero/storage/HAQZFS2R/Deng et al. - 2023 - Mind2Web Towards a Generalist Agent for the Web.pdf:application/pdf;Snapshot:/Users/anhnhat/Zotero/storage/ZX7TRWDC/2306.html:text/html},
}

@misc{li_effects_2024,
	title = {On the {Effects} of {Data} {Scale} on {UI} {Control} {Agents}},
	url = {http://arxiv.org/abs/2406.03679},
	doi = {10.48550/arXiv.2406.03679},
	abstract = {Autonomous agents that control computer interfaces to accomplish human tasks are emerging. Leveraging LLMs to power such agents has been of special interest, but unless fine-tuned on human-collected task demonstrations, performance is still relatively low. In this work we study whether fine-tuning alone is a viable approach for building real-world computer control agents. In particularly, we investigate how performance measured on both high and low-level tasks in domain and out of domain scales as more training data is collected. To this end we collect and release a new dataset, AndroidControl, consisting of 15,283 demonstrations of everyday tasks with Android apps. Compared to existing datasets, each AndroidControl task instance includes both high and low-level human-generated instructions, allowing us to explore the level of task complexity an agent can handle. Moreover, AndroidControl is the most diverse computer control dataset to date, including 14,548 unique tasks over 833 Android apps, thus allowing us to conduct in-depth analysis of the model performance in and out of the domain of the training data. Using the dataset, we find that when tested in domain fine-tuned models outperform zero and few-shot baselines and scale in such a way that robust performance might feasibly be obtained simply by collecting more data. Out of domain, performance scales significantly more slowly and suggests that in particular for high-level tasks, fine-tuning on more data alone may be insufficient for achieving robust out-of-domain performance.},
	urldate = {2025-03-14},
	publisher = {arXiv},
	author = {Li, Wei and Bishop, William and Li, Alice and Rawles, Chris and Campbell-Ajala, Folawiyo and Tyamagundlu, Divya and Riva, Oriana},
	month = nov,
	year = {2024},
	note = {arXiv:2406.03679 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: NeurIPS 2024 (Datasets and Benchmarks)},
	file = {Full Text PDF:/Users/anhnhat/Zotero/storage/4QB36DUY/Li et al. - 2024 - On the Effects of Data Scale on UI Control Agents.pdf:application/pdf;Snapshot:/Users/anhnhat/Zotero/storage/5FAX3AS7/2406.html:text/html},
}

@misc{kapoor_omniact_2024,
	title = {{OmniACT}: {A} {Dataset} and {Benchmark} for {Enabling} {Multimodal} {Generalist} {Autonomous} {Agents} for {Desktop} and {Web}},
	shorttitle = {{OmniACT}},
	url = {http://arxiv.org/abs/2402.17553},
	doi = {10.48550/arXiv.2402.17553},
	abstract = {For decades, human-computer interaction has fundamentally been manual. Even today, almost all productive work done on the computer necessitates human input at every step. Autonomous virtual agents represent an exciting step in automating many of these menial tasks. Virtual agents would empower users with limited technical proficiency to harness the full possibilities of computer systems. They could also enable the efficient streamlining of numerous computer tasks, ranging from calendar management to complex travel bookings, with minimal human intervention. In this paper, we introduce OmniACT, the first-of-a-kind dataset and benchmark for assessing an agent's capability to generate executable programs to accomplish computer tasks. Our scope extends beyond traditional web automation, covering a diverse range of desktop applications. The dataset consists of fundamental tasks such as "Play the next song", as well as longer horizon tasks such as "Send an email to John Doe mentioning the time and place to meet". Specifically, given a pair of screen image and a visually-grounded natural language task, the goal is to generate a script capable of fully executing the task. We run several strong baseline language model agents on our benchmark. The strongest baseline, GPT-4, performs the best on our benchmark However, its performance level still reaches only 15\% of the human proficiency in generating executable scripts capable of completing the task, demonstrating the challenge of our task for conventional web agents. Our benchmark provides a platform to measure and evaluate the progress of language model agents in automating computer tasks and motivates future work towards building multimodal models that bridge large language models and the visual grounding of computer screens.},
	urldate = {2025-03-14},
	publisher = {arXiv},
	author = {Kapoor, Raghav and Butala, Yash Parag and Russak, Melisa and Koh, Jing Yu and Kamble, Kiran and Alshikh, Waseem and Salakhutdinov, Ruslan},
	month = jul,
	year = {2024},
	note = {arXiv:2402.17553 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Human-Computer Interaction},
	file = {Full Text PDF:/Users/anhnhat/Zotero/storage/GRJXD9IA/Kapoor et al. - 2024 - OmniACT A Dataset and Benchmark for Enabling Mult.pdf:application/pdf;Snapshot:/Users/anhnhat/Zotero/storage/VL6PPTGI/2402.html:text/html},
}

@inproceedings{pan_webcanvas_2024,
	title = {{WebCanvas}: {Benchmarking} {Web} {Agents} in {Online} {Environments}},
	shorttitle = {{WebCanvas}},
	url = {https://openreview.net/forum?id=O1FaGasJob},
	abstract = {For web agents to be practically useful, they need to generalize to the ever changing web environment --- UI updates, page content updates, etc. Unfortunately, most traditional benchmarks only capture a static state of the web page. We introduce WebCanvas, an innovative online evaluation framework for web agents designed to address the dynamic nature of web interactions. WebCanvas contains three main components supporting realistic assessments: (1) A key-node-based evaluation metric, which stably capture critical actions or states necessary for task completions while disregarding noises caused by insignificant events or changed web-elements; (2) A benchmark dataset called Mind2Web-Live, a refined version of original Mind2Web static dataset containing 542 tasks with 2439 intermediate evaluation states; (3) Lightweight and generalizable annotation tools and testing pipelines, which allows us to maintain the high-quality, up-to-date dataset and automatically detection shifts in live action sequences. Despite the advancements, best-performing model achieves only a 23.1\% task success rate, highlighting substantial room for improvement in future work.},
	language = {en},
	urldate = {2025-03-14},
	author = {Pan, Yichen and Kong, Dehan and Zhou, Sida and Cui, Cheng and Leng, Yifei and Jiang, Bing and Liu, Hangyu and Shang, Yanyi and Zhou, Shuyan and Wu, Tongshuang and Wu, Zhengyang},
	month = jul,
	year = {2024},
	file = {Full Text PDF:/Users/anhnhat/Zotero/storage/P5PR964K/Pan et al. - 2024 - WebCanvas Benchmarking Web Agents in Online Envir.pdf:application/pdf},
}

@inproceedings{rawles_androidworld_2024,
	title = {{AndroidWorld}: {A} {Dynamic} {Benchmarking} {Environment} for {Autonomous} {Agents}},
	shorttitle = {{AndroidWorld}},
	url = {https://openreview.net/forum?id=il5yUQsrjC},
	abstract = {Autonomous agents that execute human tasks by controlling computers can enhance human productivity and application accessibility. However, progress in this field will be driven by realistic and reproducible benchmarks. We present AndroidWorld, a fully functional Android environment that provides reward signals for 116 programmatic tasks across 20 real-world Android apps. Unlike existing interactive environments, which provide a static test set, AndroidWorld dynamically constructs tasks that are parameterized and expressed in natural language in unlimited ways, thus enabling testing on a much larger and more realistic suite of tasks. To ensure reproducibility, each task includes dedicated initialization, success-checking, and tear-down logic, which modifies and inspects the device’s system state. We experiment with baseline agents to test AndroidWorld and provide initial results on the benchmark. Our best agent can complete 30.6\% of AndroidWorld's tasks, leaving ample room for future work. Furthermore, we adapt a popular desktop web agent to work on Android, which we find to be less effective on mobile, suggesting future research is needed to achieve universal, cross-platform agents. Finally, we also conduct a robustness analysis, showing that task variations can significantly affect agent performance, demonstrating that without such testing, agent performance metrics may not fully reflect practical challenges. AndroidWorld and the experiments in this paper are available at https://github.com/google-research/android\_world.},
	language = {en},
	urldate = {2025-03-14},
	author = {Rawles, Christopher and Clinckemaillie, Sarah and Chang, Yifan and Waltz, Jonathan and Lau, Gabrielle and Fair, Marybeth and Li, Alice and Bishop, William E. and Li, Wei and Campbell-Ajala, Folawiyo and Toyama, Daniel Kenji and Berry, Robert James and Tyamagundlu, Divya and Lillicrap, Timothy P. and Riva, Oriana},
	month = oct,
	year = {2024},
	file = {Full Text PDF:/Users/anhnhat/Zotero/storage/3DJQKYK8/Rawles et al. - 2024 - AndroidWorld A Dynamic Benchmarking Environment f.pdf:application/pdf},
}
